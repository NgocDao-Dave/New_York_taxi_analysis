{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cdedc500-0f88-4dd2-ab2e-5824abd74582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Modeling\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e094e3a0-0293-4c75-9f1a-495aed4c0591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " PULocationID           | 257                  \n",
      " date                   | 2024-05-04           \n",
      " hour_bucket            | 12                   \n",
      " market_share_yellow    | 0.6134969325153374   \n",
      " temp                   | 0.5845323741007195   \n",
      " rhum                   | 0.574898785425101    \n",
      " prcp                   | 0.0                  \n",
      " walkup_apartment       | 0.38260763437633494  \n",
      " elevator_apartment     | 0.1541458552776615   \n",
      " garage_n_gas           | 0.08240738669733964  \n",
      " hotel                  | 0.0                  \n",
      " hospital               | 0.0                  \n",
      " theatre                | 0.0                  \n",
      " store                  | 0.07730168860951439  \n",
      " church                 | 0.31566503937379364  \n",
      " asylum                 | 0.006835418499575569 \n",
      " office                 | 0.003478216697113... \n",
      " public_n_cultural      | 0.03875960932793963  \n",
      " condo                  | 0.055875674247014936 \n",
      " multiple_use_residence | 0.39704492806527103  \n",
      " transport              | 0.0                  \n",
      " utility                | 0.0                  \n",
      " vacant_land            | 0.020098206696561493 \n",
      " education              | 0.09025235559094517  \n",
      " government             | 4.75247280070707E-4  \n",
      " miscellanous           | 9.19128239656747E-4  \n",
      " avg_land_value         | 0.02986955624321935  \n",
      " family_dwelling        | 0.5246893214711605   \n",
      " industrial             | 0.052463901649641674 \n",
      " day_of_week            | 7                    \n",
      " month                  | 5                    \n",
      "-RECORD 1--------------------------------------\n",
      " PULocationID           | 255                  \n",
      " date                   | 2024-05-04           \n",
      " hour_bucket            | 12                   \n",
      " market_share_yellow    | 0.15420200462606012  \n",
      " temp                   | 0.5845323741007195   \n",
      " rhum                   | 0.574898785425101    \n",
      " prcp                   | 0.0                  \n",
      " walkup_apartment       | 0.2937646005671107   \n",
      " elevator_apartment     | 0.12155580721544208  \n",
      " garage_n_gas           | 0.052620431332710646 \n",
      " hotel                  | 0.10057846395848051  \n",
      " hospital               | 0.002025507310872... \n",
      " theatre                | 0.03256826653398472  \n",
      " store                  | 0.28559151846552866  \n",
      " church                 | 0.10435577546008899  \n",
      " asylum                 | 0.019935045667757507 \n",
      " office                 | 0.050080899892202255 \n",
      " public_n_cultural      | 0.01556772866953488  \n",
      " condo                  | 0.2193392317226684   \n",
      " multiple_use_residence | 0.6611089327853732   \n",
      " transport              | 0.04603626279743666  \n",
      " utility                | 0.019639675300598177 \n",
      " vacant_land            | 0.09057966426153795  \n",
      " education              | 0.04430982182708399  \n",
      " government             | 0.001282022572554639 \n",
      " miscellanous           | 0.007811193717902... \n",
      " avg_land_value         | 0.0661807919456911   \n",
      " family_dwelling        | 0.02047561197427935  \n",
      " industrial             | 0.17244134271942646  \n",
      " day_of_week            | 7                    \n",
      " month                  | 5                    \n",
      "-RECORD 2--------------------------------------\n",
      " PULocationID           | 236                  \n",
      " date                   | 2024-05-04           \n",
      " hour_bucket            | 12                   \n",
      " market_share_yellow    | 45.30020703933747    \n",
      " temp                   | 0.5845323741007195   \n",
      " rhum                   | 0.574898785425101    \n",
      " prcp                   | 0.0                  \n",
      " walkup_apartment       | 0.24012412496623411  \n",
      " elevator_apartment     | 0.5712822112469754   \n",
      " garage_n_gas           | 0.029036253710943632 \n",
      " hotel                  | 0.017880602246176915 \n",
      " hospital               | 0.009291135416917791 \n",
      " theatre                | 0.0                  \n",
      " store                  | 0.07657257243133009  \n",
      " church                 | 0.35606365781106347  \n",
      " asylum                 | 0.009909754674865722 \n",
      " office                 | 0.025369613066316153 \n",
      " public_n_cultural      | 0.17194052416269814  \n",
      " condo                  | 0.13403105104263907  \n",
      " multiple_use_residence | 0.2969653923258824   \n",
      " transport              | 0.0                  \n",
      " utility                | 0.0                  \n",
      " vacant_land            | 0.001333858544772... \n",
      " education              | 0.17905395009143024  \n",
      " government             | 8.16711996750763E-4  \n",
      " miscellanous           | 0.005349667992578832 \n",
      " avg_land_value         | 0.29047794389859133  \n",
      " family_dwelling        | 0.12800344558271085  \n",
      " industrial             | 0.0                  \n",
      " day_of_week            | 7                    \n",
      " month                  | 5                    \n",
      "-RECORD 3--------------------------------------\n",
      " PULocationID           | 234                  \n",
      " date                   | 2024-05-04           \n",
      " hour_bucket            | 12                   \n",
      " market_share_yellow    | 34.16815742397138    \n",
      " temp                   | 0.5845323741007195   \n",
      " rhum                   | 0.574898785425101    \n",
      " prcp                   | 0.0                  \n",
      " walkup_apartment       | 0.04417468794887653  \n",
      " elevator_apartment     | 0.1847033676602264   \n",
      " garage_n_gas           | 0.08412932287149266  \n",
      " hotel                  | 0.1543206435507511   \n",
      " hospital               | 0.0                  \n",
      " theatre                | 0.0                  \n",
      " store                  | 0.3001868452045597   \n",
      " church                 | 0.1407570394746784   \n",
      " asylum                 | 0.024720059074047427 \n",
      " office                 | 0.63567806793973     \n",
      " public_n_cultural      | 0.07071685788038684  \n",
      " condo                  | 0.2353204291480654   \n",
      " multiple_use_residence | 0.1915738450161391   \n",
      " transport              | 0.0                  \n",
      " utility                | 0.0                  \n",
      " vacant_land            | 0.004373202970377163 \n",
      " education              | 0.04304124348993588  \n",
      " government             | 4.20021525300787E-4  \n",
      " miscellanous           | 0.005526182340266649 \n",
      " avg_land_value         | 0.45319161264860064  \n",
      " family_dwelling        | 0.001330379222417216 \n",
      " industrial             | 0.005478194361778576 \n",
      " day_of_week            | 7                    \n",
      " month                  | 5                    \n",
      "-RECORD 4--------------------------------------\n",
      " PULocationID           | 228                  \n",
      " date                   | 2024-05-04           \n",
      " hour_bucket            | 12                   \n",
      " market_share_yellow    | 0.0                  \n",
      " temp                   | 0.5845323741007195   \n",
      " rhum                   | 0.574898785425101    \n",
      " prcp                   | 0.0                  \n",
      " walkup_apartment       | 0.27295597640740477  \n",
      " elevator_apartment     | 0.012821551494341138 \n",
      " garage_n_gas           | 0.22202818395672852  \n",
      " hotel                  | 0.03531260896888573  \n",
      " hospital               | 0.03296034696330983  \n",
      " theatre                | 0.003958960359059337 \n",
      " store                  | 0.2457750493332631   \n",
      " church                 | 0.1131509303441073   \n",
      " asylum                 | 0.03792446198196826  \n",
      " office                 | 0.056189422700484916 \n",
      " public_n_cultural      | 0.006065745630495... \n",
      " condo                  | 0.023392481890640537 \n",
      " multiple_use_residence | 0.3818616041446289   \n",
      " transport              | 0.16286277451946152  \n",
      " utility                | 0.23085001797361404  \n",
      " vacant_land            | 0.03027706273272088  \n",
      " education              | 0.04701208939869096  \n",
      " government             | 0.024057866980314646 \n",
      " miscellanous           | 0.014111979738734737 \n",
      " avg_land_value         | 0.02109237076552363  \n",
      " family_dwelling        | 0.13780836120413803  \n",
      " industrial             | 0.5961387004194565   \n",
      " day_of_week            | 7                    \n",
      " month                  | 5                    \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the final data for modeling\n",
    "file_path = '../data/final_data_for_modeling.parquet'\n",
    "final_sdf = spark.read.parquet(file_path)\n",
    "final_sdf.show(5, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d386e60",
   "metadata": {},
   "source": [
    "# Lasso Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e704216-2253-4cf0-ba93-c170088a09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets based on the month\n",
    "train_sdf_lasso = final_sdf.filter(final_sdf[\"month\"] != 5)  \n",
    "test_sdf_lasso = final_sdf.filter(final_sdf[\"month\"] == 5)   # Test on data from may"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f78fb5e-ecae-49d4-8f2a-1e0460cfe424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero coefficients:\n",
      "temp: 0.8847975514623099\n",
      "rhum: -0.3790170347340133\n",
      "prcp: -0.6598152673674832\n",
      "avg_land_value: 35.18929761911898\n",
      "walkup_apartment: -0.4501453550609017\n",
      "garage_n_gas: -1.301420377743595\n",
      "hotel: 9.922884947314833\n",
      "store: -0.43520079384123606\n",
      "church: -0.25067715843752597\n",
      "public_n_cultural: 0.31993269441904776\n",
      "multiple_use_residence: -0.4004804744619177\n",
      "transport: 13.429106182912735\n",
      "education: -0.1041299722791314\n",
      "miscellanous: 0.022608452155879172\n",
      "hour_bucket_onehot: 2.6528103910127836\n",
      "+--------------------+-------------------+------------------+\n",
      "|            features|market_share_yellow|        prediction|\n",
      "+--------------------+-------------------+------------------+\n",
      "|(295,[0,1,3,4,5,6...| 0.6134969325153374|2.0553982834497813|\n",
      "|(295,[0,1,3,4,5,6...|0.15420200462606012| 2.189965618149156|\n",
      "|(295,[0,1,3,4,5,6...|  45.30020703933747|38.827881945193724|\n",
      "|(295,[0,1,3,4,5,6...|  34.16815742397138|24.872241182481122|\n",
      "|(295,[0,1,3,4,5,6...|                0.0|2.0225649794456224|\n",
      "+--------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE): 4.216363281061418\n",
      "R-squared: 0.8623735741376788\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# One-Hot Encoding for categorical features\n",
    "indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + \"_index\").fit(final_sdf) \n",
    "            for col_name in [\"PULocationID\", \"hour_bucket\", \"day_of_week\"]]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol=col_name + \"_index\", outputCol=col_name + \"_onehot\") \n",
    "            for col_name in [\"PULocationID\", \"hour_bucket\", \"day_of_week\"]]\n",
    "\n",
    "# List of features (keeping PULocationID as is for grouping)\n",
    "feature_cols = [\"temp\", \"rhum\", \"prcp\", \"avg_land_value\"] + \\\n",
    "               [\"walkup_apartment\", \"elevator_apartment\", \"garage_n_gas\", \"hotel\", \"hospital\",\n",
    "                \"theatre\", \"store\", \"church\", \"asylum\", \"office\", \"public_n_cultural\", \"condo\",\n",
    "                \"multiple_use_residence\", \"transport\", \"utility\", \"vacant_land\", \"education\",\n",
    "                \"government\", \"miscellanous\", \"family_dwelling\", \"industrial\"] + \\\n",
    "               [\"PULocationID_onehot\", \"hour_bucket_onehot\", \"day_of_week_onehot\"]\n",
    "\n",
    "# Combine all features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Build a pipeline for indexing, encoding, assembling features\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "train_sdf_lasso = pipeline.fit(train_sdf_lasso).transform(train_sdf_lasso)\n",
    "test_sdf_lasso = pipeline.fit(test_sdf_lasso).transform(test_sdf_lasso)\n",
    "\n",
    "# Keep only the necessary columns for modeling and analysis\n",
    "train_sdf_lasso = train_sdf_lasso.select(\"PULocationID\", \"features\", \"market_share_yellow\")\n",
    "test_sdf_lasso = test_sdf_lasso.select(\"PULocationID\", \"features\", \"market_share_yellow\")\n",
    "\n",
    "# Train the Lasso model\n",
    "lasso = LinearRegression(featuresCol=\"features\", labelCol=\"market_share_yellow\", elasticNetParam=1.0, regParam=0.02)\n",
    "lasso_model = lasso.fit(train_sdf_lasso)\n",
    "\n",
    "# Get the coefficients and corresponding feature names\n",
    "coefficients = lasso_model.coefficients\n",
    "intercept = lasso_model.intercept\n",
    "\n",
    "# Map coefficients back to feature names\n",
    "non_zero_coefficients = [(feature, coeff) for feature, coeff in zip(feature_cols, coefficients) if coeff != 0]\n",
    "\n",
    "# Print out the non-zero coefficients\n",
    "print(\"Non-zero coefficients:\")\n",
    "for feature, coeff in non_zero_coefficients:\n",
    "    print(f\"{feature}: {coeff}\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_lasso = lasso_model.transform(test_sdf_lasso)\n",
    "\n",
    "# Show the predictions\n",
    "predictions_lasso.select(\"features\", \"market_share_yellow\", \"prediction\").show(5)\n",
    "\n",
    "# Evaluate the model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"market_share_yellow\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions_lasso)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# evaluate with other metrics like R-squared\n",
    "r2 = evaluator.evaluate(predictions_lasso, {evaluator.metricName: \"r2\"})\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea2d82",
   "metadata": {},
   "source": [
    "# Checking for zero coefficent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "505544c7-c068-4a46-b61a-c7faa007ef26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero coefficients:\n",
      "elevator_apartment: 0.0\n",
      "hospital: 0.0\n",
      "theatre: 0.0\n",
      "asylum: 0.0\n",
      "office: 0.0\n",
      "condo: 0.0\n",
      "utility: 0.0\n",
      "vacant_land: 0.0\n",
      "government: 0.0\n",
      "family_dwelling: 0.0\n",
      "industrial: 0.0\n",
      "PULocationID_onehot: 0.0\n",
      "day_of_week_onehot: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Print out the coefficients with zero values\n",
    "zero_coefficients = [(feature, coeff) for feature, coeff in zip(feature_cols, coefficients) if coeff == 0]\n",
    "\n",
    "print(\"Zero coefficients:\")\n",
    "for feature, coeff in zero_coefficients:\n",
    "    print(f\"{feature}: {coeff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b89630b8-d802-4078-9588-85e27b516c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Indexing the categorical features\n",
    "indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + \"_index\").fit(final_sdf) \n",
    "            for col_name in [\"PULocationID\", \"hour_bucket\", \"day_of_week\", \"month\"]]\n",
    "\n",
    "# Combine indexed features with numerical features\n",
    "feature_cols = [\"temp\", \"rhum\", \"prcp\", \"avg_land_value\"] + \\\n",
    "               [\"walkup_apartment\", \"elevator_apartment\", \"garage_n_gas\", \"hotel\", \"hospital\",\n",
    "                \"theatre\", \"store\", \"church\", \"asylum\", \"office\", \"public_n_cultural\", \"condo\",\n",
    "                \"multiple_use_residence\", \"transport\", \"utility\", \"vacant_land\", \"education\",\n",
    "                \"government\", \"miscellanous\", \"family_dwelling\", \"industrial\"] + \\\n",
    "               [\"PULocationID_index\", \"hour_bucket_index\", \"day_of_week_index\", \"month_index\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Build the pipeline for indexing and assembling features\n",
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "\n",
    "# Fit the pipeline on the entire data\n",
    "final_sdf = pipeline.fit(final_sdf).transform(final_sdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "255784c8-f5e6-42eb-9fda-c153679ffa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets based on the month (May is the test set)\n",
    "train_sdf_forest = final_sdf.filter(final_sdf[\"month_index\"] != 4)  \n",
    "test_sdf_forest = final_sdf.filter(final_sdf[\"month_index\"] == 4)\n",
    "\n",
    "# Keep only the features and the target variable for modeling\n",
    "train_sdf_forest = train_sdf_forest.select(\"PULocationID\", \"features\", \"market_share_yellow\")\n",
    "test_sdf_forest = test_sdf_forest.select(\"PULocationID\", \"features\", \"market_share_yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc430de",
   "metadata": {},
   "source": [
    "# Increase executor memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22ea688f-2363-4e49-b4a1-5c5044414499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x12153b890>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.memory\", \"6g\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c8522c",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edab3b8e-8d34-404a-a332-cc35ed1bc1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Trees: 20\n",
      "Best Max Depth: 7\n",
      "Best Max Bins: 380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 2.9984873443545186\n",
      "R-squared: 0.9301261010409843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Sample the training data for hyperparameter tuning\n",
    "train_sdf_reduced = train_sdf_forest.sample(fraction=0.1)\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"market_share_yellow\")\n",
    "\n",
    "# Create a parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 7]) \\\n",
    "    .addGrid(rf.maxBins, [260, 370, 380]) \\\n",
    "    .build()\n",
    "\n",
    "# Initialize the RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"market_share_yellow\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Fit the Random Forest model with cross-validation on the reduced training data\n",
    "cvModel = cv.fit(train_sdf_reduced)\n",
    "\n",
    "# Extract the best hyperparameters from cross-validation\n",
    "best_num_trees = cvModel.bestModel.getNumTrees\n",
    "best_max_depth = cvModel.bestModel.getMaxDepth()\n",
    "best_max_bins = cvModel.bestModel.getMaxBins()\n",
    "\n",
    "print(f\"Best Number of Trees: {best_num_trees}\")\n",
    "print(f\"Best Max Depth: {best_max_depth}\")\n",
    "print(f\"Best Max Bins: {best_max_bins}\")\n",
    "\n",
    "# Train the final model on the full training data using the best hyperparameters\n",
    "final_rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"market_share_yellow\",\n",
    "    numTrees=best_num_trees,\n",
    "    maxDepth=best_max_depth,\n",
    "    maxBins=best_max_bins\n",
    ")\n",
    "\n",
    "final_model = final_rf.fit(train_sdf_forest)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = final_model.transform(test_sdf_forest)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = evaluator.evaluate(predictions_rf)\n",
    "r2 = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2132b",
   "metadata": {},
   "source": [
    "# Analyze random forest's feature of importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f14b9887-004e-47d9-aa44-2103e046045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "hour_bucket_index: 0.4671\n",
      "walkup_apartment: 0.1874\n",
      "multiple_use_residence: 0.0853\n",
      "hospital: 0.0831\n",
      "day_of_week_index: 0.0665\n",
      "education: 0.0304\n",
      "hotel: 0.0153\n",
      "industrial: 0.0116\n",
      "garage_n_gas: 0.0103\n",
      "vacant_land: 0.0045\n",
      "family_dwelling: 0.0042\n",
      "utility: 0.0036\n",
      "transport: 0.0033\n",
      "church: 0.0033\n",
      "miscellanous: 0.0031\n",
      "asylum: 0.0031\n",
      "public_n_cultural: 0.0029\n",
      "elevator_apartment: 0.0025\n",
      "store: 0.0025\n",
      "condo: 0.0021\n",
      "theatre: 0.0016\n",
      "office: 0.0013\n",
      "PULocationID_index: 0.0013\n",
      "month_index: 0.0013\n",
      "government: 0.0012\n",
      "rhum: 0.0004\n",
      "prcp: 0.0003\n",
      "avg_land_value: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances from the trained Random Forest model\n",
    "importances = final_model.featureImportances\n",
    "\n",
    "# Zip the feature importances with their corresponding feature names\n",
    "feature_importance = [(feature, importance) for feature, importance in zip(feature_cols[1:], importances)]\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "feature_importance_sorted = sorted(feature_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print out the most important features\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in feature_importance_sorted:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e614d2",
   "metadata": {},
   "source": [
    "# Calculate average error per location of each models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "954b63ec-8aa4-47c0-a5cc-14d10fd61e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Calculate the error\n",
    "predictions_rf = predictions_rf.withColumn(\"error\", col(\"prediction\") - col(\"market_share_yellow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ac3e144-e0c0-4721-848c-51331e689215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean errors for each location\n",
    "mean_errors = predictions_rf.groupBy(\"PULocationID\").agg(expr(\"mean(error)\").alias(\"mean_error\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af35322a-2eeb-4ae9-8861-7fedd3e2fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the taxi zones shapefile\n",
    "sf = gpd.read_file(\"../data/raw/taxi_zones/taxi_zones.shp\")\n",
    "\n",
    "# Reproject the geometry to WGS84 (Longitude/Latitude)\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n",
    "\n",
    "# Convert mean_errors to Pandas DataFrame\n",
    "mean_errors_df = mean_errors.toPandas()\n",
    "\n",
    "# Merge with GeoDataFrame containing geographical data\n",
    "gdf = gpd.GeoDataFrame(pd.merge(mean_errors_df, sf, left_on='PULocationID', right_on='LocationID')) \\\n",
    "                        .drop('LocationID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f037f445-084e-4ae7-b95f-08e21cf5cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the error (difference between prediction and actual market share)\n",
    "predictions_lasso = predictions_lasso.withColumn(\"error\", expr(\"prediction - market_share_yellow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ce7a530-c7ca-4bc8-a7cb-a3f7bdac320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Group by PULocationID and calculate mean error\n",
    "mean_errors_lasso = predictions_lasso.groupBy(\"PULocationID\").agg(expr(\"mean(error)\").alias(\"mean_error\"))\n",
    "\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "mean_errors_lasso_df = mean_errors_lasso.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02cfe271-85f6-4ba4-b87a-7d7ba18827f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the mean_errors_lasso DataFrame with the GeoDataFrame\n",
    "gdf_lasso = gpd.GeoDataFrame(pd.merge(mean_errors_lasso_df, sf, left_on='PULocationID', right_on='LocationID')) \\\n",
    "                        .drop('LocationID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92190c",
   "metadata": {},
   "source": [
    "# Visualizing for Lasso regression average error distribution per location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1155e765-ce70-4a48-9581-8fc35c9a25e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to: ../plots/lasso_error_map.html\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from folium import Map, TileLayer, Choropleth, LayerControl\n",
    "import numpy as np\n",
    "\n",
    "# Convert the merged GeoDataFrame to GeoJSON format for Folium\n",
    "geoJSON_lasso = gdf_lasso[['PULocationID', 'geometry']].to_json()\n",
    "\n",
    "# Create the base map\n",
    "m_lasso = Map(location=[40.66, -73.94], zoom_start=10, tiles=None)\n",
    "TileLayer('CartoDB positron', name=\"Light Map\", control=False).add_to(m_lasso)\n",
    "\n",
    "# Create a custom color map that goes from blue (negative) to white (zero) to red (positive)\n",
    "colors = [(0, \"blue\"), (0.5, \"white\"), (1, \"red\")]\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"RdWhBu\", colors)\n",
    "\n",
    "# Calculate the max absolute error to make the color scale symmetric around zero\n",
    "max_error = max(abs(gdf_lasso['mean_error'].max()), abs(gdf_lasso['mean_error'].min()))\n",
    "\n",
    "# Set normalization to ensure the color map is symmetric around zero\n",
    "norm = Normalize(vmin=-max_error, vmax=max_error)\n",
    "\n",
    "# Convert color values into a list of RGBA values\n",
    "gdf_lasso['color'] = gdf_lasso['mean_error'].apply(lambda x: plt.cm.ScalarMappable(norm=norm, cmap=custom_cmap).to_rgba(x))\n",
    "\n",
    "# Create more bins to capture finer differences\n",
    "bins = np.linspace(-max_error, max_error, num=200)\n",
    "\n",
    "# Create the Choropleth map\n",
    "Choropleth(\n",
    "    geo_data=geoJSON_lasso,\n",
    "    name=\"choropleth\",\n",
    "    data=gdf_lasso,\n",
    "    columns=[\"PULocationID\", \"mean_error\"],\n",
    "    key_on=\"properties.PULocationID\",\n",
    "    fill_color='RdYlBu',  # Using color brewer palette, closest to your custom cmap\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.4,\n",
    "    legend_name=\"Average Prediction Error\",\n",
    "    bins=bins,\n",
    "    nan_fill_color=\"white\",\n",
    "    nan_fill_opacity=0.6\n",
    ").add_to(m_lasso)\n",
    "\n",
    "# Add layer control and save the map\n",
    "LayerControl().add_to(m_lasso)\n",
    "fpth_lasso = \"../plots/lasso_error_map.html\"\n",
    "m_lasso.save(fpth_lasso)\n",
    "\n",
    "print(f\"Map saved to: {fpth_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9215fc",
   "metadata": {},
   "source": [
    "# Visualizing for Random forest average error distribution per location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb9985ef-7220-4932-ba39-7e45b7697884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to: ../plots/mean_error_map_random_forest.html\n"
     ]
    }
   ],
   "source": [
    "# Convert the merged GeoDataFrame to GeoJSON format for Folium\n",
    "geoJSON_rf = gdf[['PULocationID', 'geometry']].to_json()\n",
    "\n",
    "# Create the base map for Random Forest\n",
    "m_rf = Map(location=[40.66, -73.94], zoom_start=10, tiles=None)\n",
    "TileLayer('CartoDB positron', name=\"Light Map\", control=False).add_to(m_rf)\n",
    "\n",
    "# Set normalization with balanced extremes for Random Forest\n",
    "max_abs_error_rf = max(abs(gdf['mean_error'].min()), abs(gdf['mean_error'].max()))\n",
    "norm_rf = Normalize(vmin=-max_abs_error_rf, vmax=max_abs_error_rf)\n",
    "\n",
    "# Convert color values into a list of RGBA values for Random Forest\n",
    "gdf['color'] = gdf['mean_error'].apply(lambda x: plt.cm.ScalarMappable(norm=norm_rf, cmap=custom_cmap).to_rgba(x))\n",
    "\n",
    "# Create more bins to capture finer differences \n",
    "bins_rf = np.linspace(-max_abs_error_rf, max_abs_error_rf, num=200)\n",
    "\n",
    "# Create the Choropleth map for Random Forest\n",
    "Choropleth(\n",
    "    geo_data=geoJSON_rf,\n",
    "    name=\"choropleth\",\n",
    "    data=gdf,\n",
    "    columns=[\"PULocationID\", \"mean_error\"],\n",
    "    key_on=\"properties.PULocationID\",\n",
    "    fill_color='RdYlBu',\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.4,\n",
    "    legend_name=\"Average Prediction Error\",\n",
    "    bins=bins_rf,\n",
    "    nan_fill_color=\"white\",\n",
    "    nan_fill_opacity=0.6\n",
    ").add_to(m_rf)\n",
    "\n",
    "# Add layer control and save the map for Random Forest\n",
    "LayerControl().add_to(m_rf)\n",
    "fpth_rf = \"../plots/mean_error_map_random_forest.html\"\n",
    "m_rf.save(fpth_rf)\n",
    "\n",
    "print(f\"Map saved to: {fpth_rf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
